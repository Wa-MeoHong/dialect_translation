import torch
import os.path as osp
import sys
import fire
import pandas as pd
import numpy as np
import torch
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, pipeline
from Speechs import *
from params import *
# Alpaca-QLoRA util file ë¡œë“œìœ„í•œ ê²½ë¡œ
from utils.callbacks import Iteratorize, Stream
from utils.prompter import Prompter
from utils.smart_tokenizer import smart_tokenizer_and_embedding_resize
import gradio as gr

"""
# í†µì‹ ì„ ìœ„í•´ ì„œë²„ë¥¼ ì—´ì–´ì•¼ í•¨.
from socket import *

HOST = str(gethostbyname(gethostname()))
HOSTNAME = gethostname()
print(HOST, ", ",HOSTNAME)     # ì„œë²„ì˜ IP ì£¼ì†Œ ì¶œë ¥(í´ë¼ì´ì–¸íŠ¸ ì ‘ì†ìš©)
# ë§ˆì´í¬ë¡œ ì…ë ¥í•œ ë¬¸ìì—´ì„ ì„œë²„ë¡œ ì „ì†¡í•˜ê¸° ìœ„í•´, ì†Œì¼“ í†µì‹ ì„ ì´ìš©
serverSock = socket(AF_INET, SOCK_STREAM)
serverSock.bind((HOSTNAME, 9998))        # ì„œë²„ ë°”ì¸ë”©. (ip, port)íŠœí”Œ

print("ì ‘ì† ëŒ€ê¸°ì¤‘")
serverSock.listen(1)
connectionSock, addr = serverSock.accept()

# í´ë¼ì´ì–¸íŠ¸ ì†Œì¼“ì´ ì„œë²„ì— ì ‘ì†í•  ê²½ìš°, ìƒˆë¡œìš´ ì†Œì¼“ê³¼, ipì£¼ì†Œë¥¼ ë°›ìŒ.
print(str(addr),'ì—ì„œ ì ‘ì†í•˜ì˜€ìŠµë‹ˆë‹¤!')
"""
"""
ip = str(input("ì„œë²„ì˜ IPì£¼ì†Œë¥¼ ì…ë ¥í•˜ì‹œì˜¤ : "))
clientSock = socket(AF_INET, SOCK_STREAM)
print("ì ‘ì† ìš”ì²­")
clientSock.connect((ip, 9998))
print("ì ‘ì† ì™„ë£Œ!")
"""

 # í˜ì˜¤ ë°œì–¸ ìŠ¤ì½”ì–´ 
hate_scores = 0.0

def main(
    prompt_template : str = "custom_template",
    base_model : str = "/content/drive/MyDrive/KB ê³µëª¨ì „/polyglot-ko-12.8b",
    lora_weights : str = 'Meohong/Dialect-Polyglot-12.8b-QLoRA',
    max_new_tokens : int = 64
):
    
    prompter = Prompter(prompt_template)                         # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    # path = osp.dirname(osp.dirname(osp.realpath(__file__)))
    # sys.path.append(f"{path}")
    tokenizer = AutoTokenizer.from_pretrained(base_model)     # í† í¬ë‚˜ì´ì € ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        #load_in_8bit=load_8bit,
        load_in_4bit=True,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
        )# ëª¨ë¸ ë¡œë“œ
    # PEFT ëª¨ë¸ë¡œ ë¡œë“œ
    if lora_weights is not None:
        model = PeftModel.from_pretrained(
        model,
        lora_weights,
        torch_dtype=torch.float16,
        # device_map={'': 0}
    )
    
    # í˜ì˜¤ ë°œì–¸ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
    hate_pipe = pipeline("text-classification", model="jh0802/Korean-Hate-KCBERT-base")
    
    # unwind broken decapoda-research config
    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2

    # í˜ì˜¤ ë°œì–¸ ì ìˆ˜ ì²´í¬ í•¨ìˆ˜
    def hate_speechs(info):
        global hate_scores
        if info["label"] == "Hate": 
            hate_scores = hate_scores + info["score"]
            strings = "ìš•ì„¤/ë¹„í•˜ ë°œì–¸ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
            
            if hate_scores >= 3.0:
                strings = "ìš•ì„¤/ë¹„í•˜ ë°œì–¸ ì§€ìˆ˜ê°€ 3ì ì´ ë„˜ì–´ í†µí™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."
                hate_scores = 0.0
                
            return hate_scores, strings
        else:
            strings = "ìš•ì„¤/ë¹„í•˜ ë°œì–¸ X"
            return hate_scores, strings

    # ëª¨ë¸ì´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ ë²ˆì—­ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ 
    def evaluate(
        input=None,
        temperature=0.1,
        top_p=0.75,
        top_k=40,
        num_beams=4,
        max_new_tokens=max_new_tokens,
        **kwargs,
    ):   
        # ê³µìš© instructionì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ë”°ë¡œ ì´ë ‡ê²Œ ì§€ì •
        instruction = "ì‚¬íˆ¬ë¦¬ê°€ í¬í•¨ëœ ë¬¸ì¥ì´ë©´ í‘œì¤€ì–´ë¡œ ë³€í™˜í•´ì£¼ì‹œì˜¤."
        prompt = prompter.generate_prompt(instruction, input)       # í”„ë¡¬í”„íŠ¸ì— ë§ê²Œ, input id, attention mask, position encoding ì§„í–‰
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"].to(device)
        # ìƒì„±ì„ í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì • 
        generation_config = GenerationConfig(
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                num_beams=num_beams,
                **kwargs,
            )
        
        # ë‹µë³€ ìƒì„± ( ì‚¬íˆ¬ë¦¬ ë²ˆì—­ )
        generation_output = model.generate(
                    input_ids=input_ids,
                    generation_config=generation_config,
                    return_dict_in_generate=True,
                    output_scores=True,
                    max_new_tokens=max_new_tokens,
                )
        # ì‚¬íˆ¬ë¦¬ ë²ˆì—­ í›„, í† í°í™”ëœ ë‹¨ì–´ ë¬¸ì¥ì„ ë“¤ê³ ì˜´
        s = generation_output.sequences[0]
        output = tokenizer.decode(s)        # í† í°í™”(ìˆ«ìí™”)ëœ ë¬¸ì¥ Decode
        output = output.replace("<|endoftext|>", "")        # ë§¨ ë§ˆì§€ë§‰ì— ì˜¤ëŠ” EOSí† í° ì‚­ì œ
        result = prompter.get_response(output)              # ë§¨ ë§ˆì§€ë§‰ì— ë‚˜ì˜¤ëŠ” ë²ˆì—­ë¬¸ë§Œ ê³¨ë¼ëƒ„
        
        # í˜ì˜¤ ë°œì–¸ íƒì§€ë¥¼ ìœ„í•´ pipelineìœ¼ë¡œ ë„˜ê¹€.
        # ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸í˜•íƒœë¡œ ë°˜í™˜ë˜ì–´ ìš”ì†Œë§Œ ë“¤ê³ ì˜´
        hate_predict = hate_pipe(result)[0]
        scores, strings = hate_speechs(hate_predict) # ì ìˆ˜ë¥¼ ê³„ì‚°í•¨.
        
        # yieldë¡œ ë„˜ê²¨ì•¼ gradioì—ì„œ ì¶œë ¥ ê°€ëŠ¥
        yield result, strings, str(scores)
    
    gr.Interface(
        fn=evaluate,
        
        # Input í•˜ëŠ” ê³³
        inputs=[
            gr.components.Textbox(lines=2, label="Input", placeholder="none")
        ],
        # ì¶œë ¥ í•˜ëŠ” ê³³, ìœ„ì—ì„œë¶€í„° ë²ˆì—­ë³¸, íƒì§€ ê²°ê³¼, í˜ì˜¤ë°œì–¸ ìŠ¤ì½”ì–´ 
        outputs=[
            gr.inputs.Textbox(
                lines=2,
                label="Output",
            ),
            gr.inputs.Textbox(
                lines=2,
                label="íƒì§€"  
            ),
            gr.inputs.Textbox(
                lines=1,
                label="Hate score"
            )
        ],
        title="ğŸŒ² KB-Dialect ",
        description="í•œêµ­ì–´ ì‚¬íˆ¬ë¦¬ ë²ˆì—­ê³¼, ëˆ„ì  Hate Scoreë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.",  # noqa: E501
    ).queue().launch(server_name="0.0.0.0", share=True)
    
if __name__ == "__main__":
    fire.Fire(main)
